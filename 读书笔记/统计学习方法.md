# 统计学习方法
## 1 统计学习方法概论
### 1.1 统计学习
1. 统计学习的特点
	1. 以计算机及网络为平台
	2. 以数据为研究对象
	3. 目的是对数据进行预测与分析
	4. 交叉学科
2. 统计学习的对象
	1. 是数据
3. 统计学习的目的
4. 统计学习的方法
	1. 主要有
		1. 监督学习（本书主要讨论）
		2. 非监督学习
		3. 半监督学习
		4. 强化学习
	2. 三要素
		1. 模型
		2. 策略
		3. 算法
	3. 实现步骤
		1. 得到一个训练数据集合
		2. 确定学习模型的集合
		3. 确定学习的策略
		4. 确定学习的算法
		5. 通过学习方法选择最优模型
		6. 利用学习的最优模型对新数据进行预测或分析
5. 统计学习的研究
	1. 方法
	2. 理论
	3. 应用
6. 统计学习的重要性
### 1.2 监督学习
#### 1.2.1 基本概念
1. 输入空间、特征空间与输出空间
	1. 每个输入是一个实例，通常由特征向量表示
	2. 监督学习从训练数据集合中学习模型，对测试数据进行预测
	3. 根据输入变量和输出变量的不同类型
		1. 回归问题: 都连续
		2. 分类问题: 输出有限离散
		3. 标注问题: 都是变量序列
2. 联合概率分布
3. 假设空间
	1. 模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间
	2. 模型可以是（非）概率模型
#### 1.2.2 问题的形式化
![QQ_1725975153680.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102132968.png)
### 1.3 统计学习三要读
- 方法=模型+策略+算法
#### 1.3.1 模型
- 模型就是索要学习的条件概率分布或决策函数
$$
\mathcal{F}=\{f\mid Y=f(X)\}
$$
- 参数空间
$$
\mathcal{F}=\{f | Y=f_{\theta}(X),\theta\in\mathbf{R}^{n}\}
$$
- 同样可以定义为条件概率的集合
$$
\mathcal{F}=\{P|P(Y|X)\}
$$
$$
\mathcal{F}=\{P\mid P_{\theta}(Y\mid X),\theta\in\mathbf{R}^{n}\}
$$
#### 1.3.2 策略
1. 损失函数和风险函数
	1. loos function or cost function $\displaystyle L(Y,f(X))$
		1. 0-1 loss function
			1. $\displaystyle L(Y,f(X))=\begin{cases}1,&Y\neq f(X)\\0,&Y=f(X)\end{cases}$
		2. quadratic loss function
			1. $\displaystyle L(Y,f(X))=(Y-f(X))^{2}$
		3. absolute loss function
			1. $\displaystyle L(Y,f(X))=|Y-f(X)|$
		4. logarithmic loss function or log-likelihood loss function
			1. $\displaystyle L(Y,P(Y\mid X))=-\log P(Y\mid X)$
	2. $\displaystyle R_{\exp}(f)=E_{P}[L(Y,f(X))]=\int_{x\times y}L(y,f(x))P(x,y)\mathrm{d}x\mathrm{d}y$
		1. risk function or expected loss
		2. 但是联合分布位置，所以要学习，但是这样以来风险最小又要用到联合分布，那么这就成为了病态问题 (ill-formed problem)
	3. empirical risk or empirical loss
		1. $\displaystyle R_{\mathrm{emp}}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i}))$
		2. 当 $\displaystyle N$ 趋于无穷时，经验风险趋于期望风险
			1. 这就关系到两个基本策略:
				1. 经验风险最小化
				2. 结构风险最小化
2. 经验风险最小化与结构风险最小化
	1. empirical risk minimization （样本容量比较大的时候）
		1. $\displaystyle \min_{f\in\mathcal{F}} \frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i}))$
		2. maximum likelihood estimation
	2. structural risk minimization
		1. regularization
		2. $\displaystyle R_{\mathrm{sm}}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i}))+\lambda J(f)$
		3. 复杂度表示了对复杂模型的乘法
		4. maximum posterior probability estimation
#### 1.3.3 算法
### 1.4 模型评估与模型选择
#### 1.4.1 训练误差与测试误差
$$
R_{\mathrm{emp}}(\hat{f})=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},\hat{f}(x_{i}))
$$
$$
e_{\mathrm{test}}=\frac{1}{N^{\prime}}\sum_{i=1}^{N^{\prime}}L(y_{i},\hat{f}(x_{i}))
$$
$$
r_{\mathrm{test}}+e_{\mathrm{test}}=1
$$
- generalization ability
#### 1.4.2 过拟合与模型选择
- model selection
- over-fitting
![QQ_1725977613135.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102213396.png)
### 1.5 正则化与交叉验证
#### 1.5.1 正则化
$$
L(w)=\frac{1}{N}\sum_{i=1}^{N}(f(x_{i};w)-y_{i})^{2}+\frac{\lambda}{2}\parallel w\parallel^{2}
$$
#### 1.5.2 交叉验证
- cross validation
- 数据集
	- 训练集
	- 验证集
	- 测试集
1. 简单交叉验证
2. $\displaystyle S$ 折交叉验证
3. 留一交叉验证
### 1.6 泛化能力
#### 1.6.1 泛化误差
- generalization error
$$
R_{\exp}(\hat{f})=E_{P}[L(Y,\hat{f}(X))]=\int_{R\times y}L(y,\hat{f}(x))P(x,y)\mathrm{d}x\mathrm{d}y
$$
#### 1.6.2 泛化误差上界
- generalization error bound
- 样本容量增加时，泛化上界趋于 0
- 假设空间越大，泛化误差上界越大
![QQ_1725978149442.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102222045.png)
- 这个定理只适用于假设空间包含有限个函数
### 1.7 生成模型与判别模型
- generative model
	- 还原出联合概率分布 $\displaystyle P(X,Y)$
	- 朴素贝叶斯法
	- 隐马尔可夫模型
	- 收敛速度快
- discriminative model
	- 直接学习决策函数或条件概率分布 $\displaystyle P(Y|X)$
	- $\displaystyle k$ 近邻法
	- 感知机
	- 决策树
	- 逻辑斯谛回归模型
	- 最大熵模型
	- 支持向量机
	- 提升方法
	- 条件随机场
	- 准确度高
### 1.8 分类问题
- precision $\displaystyle P=\frac{TP}{TP+FP}$
- recall $\displaystyle R=\frac{TP}{TP+FN}$
![QQ_1725979882159.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102251734.png)
$$
\frac{2}{F_{1}}=\frac{1}{P}+\frac{1}{R}
$$
$$
F_{1}=\frac{2TP}{2TP+FP+FN}
$$
- text classification
### 1.9 标注问题
- tagging 是 classificationd 一个推广
- 是 structure prediction 的简单形式
- 隐马尔可夫模型
- 条件随机场
### 1.10 回归问题
- regression
- （非）线性回归，一元回归，多元回归
## 2 感知机
- perception
- 感知机对应于输入空间中将实例划分成正负两类的分离超平面，属于判别模型
- 原始形式和对偶形式
### 2.1 感知机模型
![QQ_1725980556672.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102302217.png)
- 假设空间是定义在特征空间中所有的线性分类模型（linear classification model）$\displaystyle \{f|f(x) = w \cdot x+b\}$ 
- separating hyperplane
![QQ_1725980719817.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102305607.png)
### 2.2 感知机学习策略
#### 2.2.1 数据集的线性可分性
![QQ_1725980832517.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102307386.png)
#### 2.2.2 感知机学习策略
- 定义损失函数并将损失函数极小化
$$
L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)
$$
#### 2.2.3 感知机学习算法
#### 2.2.4 感知机学习算法的原始形式
$$
\min_{w,b}L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)
$$
- stochastic gradient descent
$$
\nabla_{_w}L(w,b)=-\sum_{x_{i}\in M}y_{i}x_{i}
$$
$$
\nabla_{b}L(w,b)=-\sum_{x_{i}eM}y_{i}
$$
$$
w\leftarrow w+\eta y_{i}x_{i}
$$
$$
b\leftarrow b+\eta y_{i}
$$
- $\displaystyle \eta$ 被称为学习率（learning rate）
![QQ_1725981107428.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102311433.png)
#### 2.2.5 算法的收敛性
![QQ_1725981340195.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102315910.png)
- 为了得到唯一的超平面，需要对分离超平面增加约束条件，即线性支持向量机
- 如果训练集线性不可分，那么感知机学习算法不收敛
#### 2.2.6 感知机学习算法的对偶形式
$$
\begin{aligned}&w\leftarrow w+\eta y_{i}x_{i}\\&b\leftarrow b+\eta y_{i}\end{aligned}
$$
$$
w=\sum_{i=1}^{N}\alpha_{i}y_{i}x_{i}
$$
$$
b=\sum_{i=1}^{N}\alpha_{i}y_{i}
$$
![QQ_1725982357513.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102332751.png)
![QQ_1725982366353.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102332104.png)
- Gram matrix
$$
G=[x_{i}\cdot x_{j}]_{N\times N}
$$
## 3 $\displaystyle k$ 近邻法
- k-nearest neighbor
### 3.1 $\displaystyle k$ 近邻算法
![QQ_1725982597756.png](https://cdn.jsdelivr.net/gh/WncFht/picture/202409102336466.png)

### 3.2 $\displaystyle k$ 近邻模型
#### 3.2.1 模型
- cell
- class label



## 4 朴素贝叶斯法
## 5 决策树
## 6 逻辑斯谛回归与最大熵模型
## 7 支持向量机
## 8 提升方法
## 9 $\displaystyle \boldsymbol{EM}$ 算法及其推广
## 10 隐马尔可夫模型
## 11 条件随机场 
